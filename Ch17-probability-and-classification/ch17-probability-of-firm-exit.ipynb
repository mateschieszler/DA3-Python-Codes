{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 17\n",
    "\n",
    "**Ch17 Predicting probability of firm exit**\n",
    "\n",
    "using the bosnide dataset\n",
    "\n",
    "version 1.0 2024-01-15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import patsy\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import brier_score_loss, roc_curve, auc, confusion_matrix, roc_auc_score, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from plotnine import *\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_results(y_true, y_pred):\n",
    "\n",
    "    # Regression metrics\n",
    "    explained_variance=metrics.explained_variance_score(y_true, y_pred)\n",
    "    mean_absolute_error=metrics.mean_absolute_error(y_true, y_pred) \n",
    "    mse=metrics.mean_squared_error(y_true, y_pred) \n",
    "    median_absolute_error=metrics.median_absolute_error(y_true, y_pred)\n",
    "    r2=metrics.r2_score(y_true, y_pred)\n",
    "\n",
    "    print('explained_variance: ', round(explained_variance,4))    \n",
    "    print('r2: ', round(r2,4))\n",
    "    print('MAE: ', round(mean_absolute_error,4))\n",
    "    print('MSE: ', round(mse,4))\n",
    "    print('RMSE: ', round(np.sqrt(mse),4))\n",
    "    \n",
    "def create_coef_matrix(X, model):\n",
    "    coef_matrix = pd.concat(\n",
    "        [pd.DataFrame(X.columns),pd.DataFrame(model.coef_.flatten())], axis = 1\n",
    "    )\n",
    "    coef_matrix.columns = ['variable', 'coefficient']\n",
    "    coef_matrix.iloc[-1] = ['Intercept', model.intercept_.flatten()[0]]\n",
    "    return coef_matrix\n",
    "\n",
    "def cv_summary(lambdas, C_values, model):\n",
    "    d = {'lambdas': lambdas, 'C_values': C_values, 'mean_cv_score': model.scores_[1].mean(axis = 0)}\n",
    "    return(pd.DataFrame(data=d))\n",
    "\n",
    "def create_roc_plot(y_true, y_pred):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "    all_coords = pd.DataFrame({\n",
    "        'fpr': fpr,\n",
    "        'tpr': tpr,\n",
    "        'thresholds': thresholds\n",
    "    })\n",
    "    \n",
    "    plot = ggplot(all_coords, aes(x = 'fpr', y = 'tpr')) \\\n",
    "        + geom_line(color=color[0], size = 0.7) \\\n",
    "        + geom_area(position = 'identity', fill = 'mediumaquamarine', alpha = 0.3) \\\n",
    "        + xlab(\"False Positive Rate (1-Specifity)\") \\\n",
    "        + ylab(\"True Positive Rate (Sensitivity)\") \\\n",
    "        + geom_abline(intercept = 0, slope = 1,  linetype = \"dotted\", color = \"black\") \\\n",
    "        + scale_y_continuous(limits = (0, 1), breaks = seq(0, 1, .1), expand = (0, 0.01)) \\\n",
    "        + scale_x_continuous(limits = (0, 1), breaks = seq(0, 1, .1), expand = (0.01, 0)) \\\n",
    "        + theme_bw()\n",
    "    return(plot)\n",
    "\n",
    "def sigmoid_array(x):\n",
    "    return(1 / (1 + np.exp(-x)))\n",
    "\n",
    "def generate_fold_prediction(model, X, fold, param_index):\n",
    "    fold_coef = model.coefs_paths_[1][fold,param_index,:]\n",
    "    return(sigmoid_array(np.dot(X, np.transpose(fold_coef)[:-1]) +  np.transpose(fold_coef)[-1]))\n",
    "\n",
    "def create_loss_plot(all_coords, optimal_threshold, curr_exp_loss):\n",
    "    all_coords_copy = all_coords.copy()\n",
    "    all_coords_copy['loss'] = (all_coords_copy.false_pos*FP + all_coords_copy.false_neg*FN)/all_coords_copy.n\n",
    "    \n",
    "    t = optimal_threshold\n",
    "    l = curr_exp_loss\n",
    "    \n",
    "    plot = ggplot(all_coords_copy, aes(x = 'thresholds', y = 'loss')) + \\\n",
    "        geom_line(color=color[0], size=0.7) + \\\n",
    "        scale_x_continuous(breaks = seq(0, 1.1, by = 0.1)) + \\\n",
    "        coord_cartesian(xlim=(0,1))+ \\\n",
    "        geom_vline(xintercept = t , color = color[0] ) + \\\n",
    "        annotate(geom = \"text\", x = t - 0.01, y= max(all_coords_copy.loss) - 0.4,\n",
    "                 label=\"best threshold: \" + str(round(t,2)),\n",
    "                 colour=color[1], angle=90, size = 7) +\\\n",
    "        annotate(geom = \"text\", x = t + 0.06, y= l,\\\n",
    "                 label= str(round(l, 2)), size = 7) +\\\n",
    "        theme_bw()\n",
    "    return(plot)\n",
    "\n",
    "\n",
    "def create_roc_plot_with_optimal(all_coords, optimal_threshold):\n",
    "    all_coords_copy = all_coords.copy()\n",
    "    all_coords_copy['sp'] = all_coords_copy.true_neg/all_coords_copy.neg\n",
    "    all_coords_copy['se'] = all_coords_copy.true_pos/all_coords_copy.pos\n",
    "    \n",
    "    best_coords = all_coords_copy[all_coords_copy.thresholds == optimal_threshold]\n",
    "    sp = best_coords.sp.values[0]\n",
    "    se = best_coords.se.values[0]\n",
    "\n",
    "    plot = ggplot(all_coords_copy, aes(x = 'sp', y = 'se')) +\\\n",
    "        geom_line(color=color[0], size=0.7) +\\\n",
    "        scale_y_continuous(breaks = seq(0, 1.1, by = 0.1)) +\\\n",
    "        scale_x_reverse(breaks = seq(0, 1.1, by = 0.1)) +\\\n",
    "        geom_point(data = pd.DataFrame({'sp': [sp], 'se': [se]})) +\\\n",
    "        annotate(geom = \"text\", x = sp, y = se + 0.03,\n",
    "                 label = str(round(sp, 2)) + ', ' + str(round(se, 2)), size = 7) +\\\n",
    "        theme_bw()\n",
    "    return(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current script and repository folder\n",
    "current_path = os.getcwd()\n",
    "repository_path = current_path.split('Ch17')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add utils folder to sys path \n",
    "# Note: os.path.join() creates a string with the right syntax for defining a path for your operating sytem.\n",
    "sys.path.append(os.path.join(repository_path, 'utils'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data folder\n",
    "data_path = os.path.join(repository_path, 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the prewritten helper functions\n",
    "from py_helper_functions import *"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# DATA IMPORT - FROM FILE\n",
    "data = pd.read_csv(os.path.join(data_path, 'bisnode_firms_clean.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA IMPORT - FROM GITHUB\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/peterduronelly/DA3-Python-Codes/main/data/bisnode_firms_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns.tolist()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Current script folder\n",
    "path = os.getcwd()\n",
    "base_dir = path.split(\"da_case_studies\")[0]\n",
    "\n",
    "#Set the location of your data directory\n",
    "data_dir = base_dir + 'da_data_repo'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# location folders\n",
    "data_in = os.path.join(data_dir, 'bisnode-firms/clean/')\n",
    "data_out = os.path.join(data_dir, 'bisnode-firms/')\n",
    "output = os.path.join(data_out, 'output/')\n",
    "func = os.path.join(base_dir, 'da_case_studies/ch00-tech-prep/')\n",
    "sys.path.append(func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define variable sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawvars = [\"curr_assets\", \"curr_liab\", \"extra_exp\", \"extra_inc\", \"extra_profit_loss\", \"fixed_assets\",\n",
    "              \"inc_bef_tax\", \"intang_assets\", \"inventories\", \"liq_assets\", \"material_exp\", \"personnel_exp\",\n",
    "              \"profit_loss_year\", \"sales\", \"share_eq\", \"subscribed_cap\"]\n",
    "qualityvars = [\"balsheet_flag\", \"balsheet_length\", \"balsheet_notfullyear\"]\n",
    "engvar = [\"total_assets_bs\", \"fixed_assets_bs\", \"liq_assets_bs\", \"curr_assets_bs\",\n",
    "            \"share_eq_bs\", \"subscribed_cap_bs\", \"intang_assets_bs\", \"extra_exp_pl\",\n",
    "            \"extra_inc_pl\", \"extra_profit_loss_pl\", \"inc_bef_tax_pl\", \"inventories_pl\",\n",
    "            \"material_exp_pl\", \"profit_loss_year_pl\", \"personnel_exp_pl\"]\n",
    "engvar2 = [\"extra_profit_loss_pl_quad\", \"inc_bef_tax_pl_quad\",\n",
    "             \"profit_loss_year_pl_quad\", \"share_eq_bs_quad\"]\n",
    "engvar3 = []\n",
    "for col in data.columns:\n",
    "    if col.endswith('flag_low') or col.endswith('flag_high') or col.endswith('flag_error') or col.endswith('flag_zero'):\n",
    "        engvar3.append(col)\n",
    "\n",
    "\n",
    "d1 =  [\"d1_sales_mil_log_mod\", \"d1_sales_mil_log_mod_sq\",\n",
    "         \"flag_low_d1_sales_mil_log\", \"flag_high_d1_sales_mil_log\"]\n",
    "hr = [\"female\", \"ceo_age\", \"flag_high_ceo_age\", \"flag_low_ceo_age\",\n",
    "        \"flag_miss_ceo_age\", \"ceo_count\", \"labor_avg_mod\",\n",
    "        \"flag_miss_labor_avg\", \"foreign_management\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.ind2_cat.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See `patsy` formula language [here](https://patsy.readthedocs.io/en/latest/formulas.html), treatment of categorical variables [here](https://patsy.readthedocs.io/en/latest/categorical-coding.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creat dummy columns from category variables and drop first level\n",
    "ind2_catmat = patsy.dmatrix(\"0 + C(ind2_cat)\",data, return_type=\"dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind2_catmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: why are we dropping the first level? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind2_catmat = ind2_catmat.drop(['C(ind2_cat)[26]'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.m_region_loc.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_region_locmat = patsy.dmatrix(\"0 + C(m_region_loc)\",data, return_type=\"dataframe\")\n",
    "m_region_locmat = m_region_locmat.drop(['C(m_region_loc)[Central]'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.urban_m.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urban_mmat = patsy.dmatrix(\"0 + C(urban_m)\",data, return_type=\"dataframe\")\n",
    "urban_mmat = urban_mmat.drop(['C(urban_m)[1]'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X1\n",
    "basevars = data[[\"sales_mil_log\", \"sales_mil_log_sq\", \"d1_sales_mil_log_mod\", \"profit_loss_year_pl\"]]\n",
    "X1 = pd.concat([basevars, ind2_catmat], axis=1)\n",
    "\n",
    "# Define X2\n",
    "X2additional_vars = data[[\"fixed_assets_bs\", \"share_eq_bs\",\"curr_liab_bs\", \"curr_liab_bs_flag_high\", \\\n",
    "                          \"curr_liab_bs_flag_error\",  \"age\", \"foreign_management\"]]\n",
    "X2 = pd.concat([X1, X2additional_vars], axis=1)\n",
    "\n",
    "# Define X3\n",
    "firm = pd.concat([data[[\"age\", \"age2\", \"new\"]], ind2_catmat, m_region_locmat, urban_mmat], axis=1)\n",
    "X3 = pd.concat([data[[\"sales_mil_log\", \"sales_mil_log_sq\"] + engvar + d1], firm], axis=1)\n",
    "\n",
    "# Define X4\n",
    "X4 = pd.concat([data[[\"sales_mil_log\", \"sales_mil_log_sq\"] + engvar + d1 \\\n",
    "                                 + engvar2 + engvar3 + hr + qualityvars], firm], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define X5\n",
    "\n",
    "#Creat matrix for interactions1 variables\n",
    "int1mat = patsy.dmatrix(\"0 + C(ind2_cat):age + C(ind2_cat):age2 + C(ind2_cat):d1_sales_mil_log_mod \\\n",
    "                + C(ind2_cat):sales_mil_log + C(ind2_cat):ceo_age + C(ind2_cat):foreign_management \\\n",
    "                + C(ind2_cat):female + C(ind2_cat):C(urban_m) + C(ind2_cat):labor_avg_mod\", \n",
    "                        data, return_type=\"dataframe\")\n",
    "\n",
    "#Drop first level to get k-1 dummies out of k categorical levels \n",
    "for col in int1mat.columns:\n",
    "    if col.startswith('C(ind2_cat)[26]') or col.endswith('C(urban_m)[1]'):\n",
    "        int1mat = int1mat.drop([col], axis=1)\n",
    "        \n",
    "#Creat matrix for interactions2 variables        \n",
    "int2mat = patsy.dmatrix(\"0 + sales_mil_log:age + sales_mil_log:female + sales_mil_log:profit_loss_year_pl \\\n",
    "                + sales_mil_log:foreign_management\", \n",
    "                        data, return_type=\"dataframe\")\n",
    "\n",
    "X5 = pd.concat([X4, int1mat, int2mat], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define logitvars for LASSO\n",
    "logitvars = pd.concat([X4, int1mat, int2mat], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define rfvars for RF (no interactions, no modified features)\n",
    "rfvars  = pd.concat([data[[\"sales_mil\", \"d1_sales_mil_log\"] + rawvars + hr + qualityvars], firm], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['default']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple linear and logistic models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simplest model: X1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_modelx1 = LinearRegression().fit(X1, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_results(y, ols_modelx1.predict(X1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_coef_matrix(X1, ols_modelx1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_modelx1 = LogisticRegression(\n",
    "    solver=\"newton-cg\",max_iter=1000, penalty=\"none\", random_state = 20240205).fit(X1, y)\n",
    "regression_results(y, glm_modelx1.predict(X1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the negative variance: see Wikipedia on [$R^2$](https://en.wikipedia.org/wiki/Coefficient_of_determination)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_coef_matrix(X1, glm_modelx1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model X2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_modelx2 = LogisticRegression(solver=\"newton-cg\", max_iter=1000, penalty=\"none\").fit(X2, y)\n",
    "regression_results(y, glm_modelx2.predict(X2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_coef_matrix(X2, glm_modelx2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx2 = sm.Logit(y,sm.add_constant(X2)).fit().get_margeff()\n",
    "print(mx2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Baseline model is X4 (all vars, but no interactions)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_model = LinearRegression().fit(X4, y)\n",
    "regression_results(y, ols_model.predict(X4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_coef_matrix(X4, ols_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_model = LogisticRegression(solver=\"newton-cg\",max_iter=1000, penalty=\"none\").fit(X4, y)\n",
    "regression_results(y, glm_model.predict(X4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_coef_matrix(X4, glm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get marginal effects\n",
    "m = sm.Logit(y,sm.add_constant(X4)).fit().get_margeff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> \n",
    "\n",
    "**Keep significant variables only**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = m.summary().tables[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = pd.DataFrame(t.data).iloc[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_marginal_effects = pd.DataFrame(t.data, columns = columns).iloc[1:].astype(\n",
    "    {'dy/dx': float, 'P>|z|': float})\n",
    "df_marginal_effects[df_marginal_effects['P>|z|'] <= 0.05].sort_values(by = 'dy/dx', ascending = False).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train and holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_train, index_holdout= train_test_split(\n",
    "    data.index.values, train_size=round(0.8*len(data.index)), random_state=42)\n",
    "\n",
    "y_train = y.iloc[index_train]\n",
    "y_holdout = y.iloc[index_holdout]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total')\n",
    "print(data['default'].value_counts(normalize=True))\n",
    "print('Train')\n",
    "print(data.iloc[index_train]['default'].value_counts(normalize=True))\n",
    "print('Holdout')\n",
    "print(data.iloc[index_holdout]['default'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions using cross-validations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <font color = ' blue'> A digression into optimization algorithms </font>\n",
    "\n",
    "\n",
    "    \n",
    "Our task is to find the best parameters that give us the least error in predicting the output. We call the function that calculates this error a **cost function**  or a **loss function**, and our goal is to minimize the error in order to get the best-predicted output. This cost function is usally **quadratic**, either globally with one optimum (simple case) or locally with multiple local optima and one global optima somewhere along the sets of parameters.\n",
    "- *newton-cg*: It uses a quadratic function minimalization and finds global optimum swiftly. Drawbacks: computationally expensive and stops at the first saddle point.\n",
    "- *newton-cholesky*: It uses a newton-cg algorithm with a Cholesky decomposition of the Hessian matrix. This solver is more efficient than \"newton-cg\" for large datasets, as it requires less memory and computational resources. However, it may not converge for some datasets.\n",
    "- *liblinear*: It is a linear classification that supports logistic regression and linear support vector machine. It applies automatic parameter selection (a.k.a L1 Regularization) and it’s recommended when you have high dimension dataset (recommended for solving large-scale classification problems).\n",
    "- *lbfgs*: It is a simplified newton-cg which performs better on a limited dataset. It, however, may not converge to anything. \n",
    "‘newton-cholesky’, \n",
    "- *sag* (stochastic average descent): It optimizes the sum of a finite number of smooth convex functions. Like stochastic gradient (SG) methods, the SAG method's iteration cost is independent of the number of terms in the sum. However, by incorporating a memory of previous gradient values, the SAG method achieves a faster convergence rate than black-box SG methods. Drawbacks: it only supports L2 regularization and it is less practical for large dataset for its memory consumption. \n",
    "- *saga* (stochastic average descent): A sag-alternative which supports L2 regularization.\n",
    "\n",
    "More [here](https://stackoverflow.com/questions/38640109/logistic-regression-python-solvers-definitions)\n",
    "\n",
    "Supported penalties by solver:\n",
    "\n",
    "- *lbfgs*: L2\n",
    "- *liblinear*: L1, L2\n",
    "- *newton-cg*: L2\n",
    "- *newton-cholesky*: L2\n",
    "- *sag*: L2\n",
    "- *saga*: elasticnet, L1, L2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### specify cross-validation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = KFold(n_splits = 5, shuffle = True, random_state = 20240205)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no regularisation needed so setting the paremeter to very high value\n",
    "Cs_value_logit = [1e20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using `LogisticRegressionCV`, a logistic regression with built-in cross-validation.\n",
    "\n",
    "Note: Each of the values in Cs describes the inverse of regularization strength. If Cs is as an int, then a grid of Cs values are chosen in a logarithmic scale between 1e-4 and 1e4. Like in support vector machines, *smaller values specify stronger regularization*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_model_vars = [X1.iloc[index_train], X2.iloc[index_train], X3.iloc[index_train], X4.iloc[index_train], X5.iloc[index_train]]\n",
    "\n",
    "logit_models = dict()\n",
    "CV_RMSE_folds = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(len(logit_model_vars)):\n",
    "    LRCV_brier = LogisticRegressionCV(\n",
    "        Cs=Cs_value_logit, \n",
    "        cv=k, # simply the number of folds\n",
    "        refit=True, \n",
    "        scoring='neg_brier_score', \n",
    "        solver=\"newton-cg\", \n",
    "        tol=1e-7, \n",
    "        random_state=20240205)\n",
    "    logit_models['X'+str(i+1)] = LRCV_brier.fit(logit_model_vars[i], y_train)\n",
    "    \n",
    "    # Calculate RMSE on test for each fold\n",
    "    CV_RMSE_folds['X'+str(i+1)] = np.sqrt(-1*(logit_models['X'+str(i+1)].scores_[1])).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(CV_RMSE_folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_logitvars = pd.DataFrame(StandardScaler().fit_transform(logitvars.iloc[index_train]))\n",
    "normalized_logitvars.columns = logitvars.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas=list(10**np.arange(-1,-4.01, -1/3))\n",
    "n_obs = normalized_logitvars.shape[0]*4/5\n",
    "Cs_values = [1/(l*n_obs) for l in lambdas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training for ***accuracy***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logLasso = LogisticRegressionCV(\n",
    "    Cs = Cs_values, \n",
    "    penalty = 'l1', # L1 makes it lasso\n",
    "    cv = k, \n",
    "    refit = True, \n",
    "    scoring = 'accuracy', \n",
    "    solver = 'liblinear',\n",
    "    random_state = 20240205)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "logit_models[\"LASSO\"] = logLasso.fit(normalized_logitvars, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_summary_lasso = cv_summary(lambdas, Cs_values, logit_models[\"LASSO\"])\n",
    "cv_summary_lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lambda = cv_summary_lasso.sort_values('mean_cv_score', ascending = False).iloc[0,0]\n",
    "best_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_coef_matrix(normalized_logitvars, logit_models[\"LASSO\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training for ***Brier-score*** (aka RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#refit with negative brier score so we have RMSE values for the same cv split\n",
    "\n",
    "logLasso_brier = LogisticRegressionCV(\n",
    "    Cs = Cs_values, \n",
    "    penalty = 'l1', \n",
    "    cv = k, \n",
    "    refit = True, \n",
    "    scoring = 'neg_brier_score', \n",
    "    solver = \"liblinear\", \n",
    "    random_state = 20240205)\n",
    "logLasso_brier_fitted = logLasso_brier.fit(normalized_logitvars, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, l in enumerate(lambdas):\n",
    "    if l == best_lambda:\n",
    "        best_lambda_i = i\n",
    "        CV_RMSE_folds['LASSO'] = np.sqrt(-1*(logLasso_brier_fitted.scores_[1][:,i])).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(CV_RMSE_folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC using no loss fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate AUC for each folds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_AUC_folds = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**refit logit models with AUC so we have AUC values for the same cv split**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training for ***AUC***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(logit_model_vars)):\n",
    "    LRCV_auc = LogisticRegressionCV(\n",
    "        Cs=Cs_value_logit, \n",
    "        cv=k, \n",
    "        refit=True, \n",
    "        scoring='roc_auc', \n",
    "        solver=\"newton-cg\", \n",
    "        tol=1e-7, \n",
    "        random_state = 20240205)\n",
    "    LRCV_auc_fit = LRCV_auc.fit(logit_model_vars[i], y_train)\n",
    "    # Calculate AUC on test for each fold\n",
    "    CV_AUC_folds['X'+str(i+1)] = LRCV_auc_fit.scores_[1][:,0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(CV_AUC_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#refit with AUC so we have AUC values for the same cv split\n",
    "\n",
    "logLasso_auc = LogisticRegressionCV(\n",
    "    Cs=Cs_values, \n",
    "    penalty='l1', \n",
    "    cv=k, \n",
    "    refit=True, scoring='roc_auc', \n",
    "    solver=\"liblinear\", \n",
    "    random_state = 20240205)\n",
    "logLasso_auc_fitted = logLasso_auc.fit(normalized_logitvars, y_train)\n",
    "CV_AUC_folds['LASSO'] = logLasso_auc_fitted.scores_[1][:,best_lambda_i].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(CV_AUC_folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For each model: average RMSE and average AUC for models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_RMSE = dict()\n",
    "CV_AUC = dict()\n",
    "nvars = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in logit_models:\n",
    "    CV_RMSE[key] = np.mean(CV_RMSE_folds[key])\n",
    "    CV_AUC[key] = np.mean(CV_AUC_folds[key])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in logit_models:\n",
    "    if key != 'LASSO':\n",
    "        nvars[key] = logit_models[key].n_features_in_\n",
    "    else:\n",
    "        nvars[key] = sum(x != 0 for x in logit_models[key].coef_[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logLasso.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "We have 6 models, (5 logit and the logit lasso). For each we have a 5-CV RMSE and AUC.\n",
    "We pick our preferred model based on that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_summary1 = np.transpose(pd.DataFrame.from_dict([nvars,CV_RMSE,CV_AUC], orient='columns'))\n",
    "logit_summary1.columns = ['Number of predictors', 'CV RMSE', 'CV AUC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_summary1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take best model and estimate RMSE on holdout\n",
    "X4, X5 and LASSO are practically the same - go with the simplest model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = logit_models['X4']\n",
    "best_model_X_holdout = X4.iloc[index_holdout]\n",
    "\n",
    "logit_predicted_probabilities_holdout = best_model.predict_proba(best_model_X_holdout)[:,1]\n",
    "best_rmse_holdout = np.sqrt(metrics.mean_squared_error(y_holdout, logit_predicted_probabilities_holdout))\n",
    "round(best_rmse_holdout, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_predicted_probabilities_holdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discrete ROC (with thresholds in steps) on holdout**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [i/100 for i in range(5,80,5)]\n",
    "cm = dict()\n",
    "true_positive_rates = []\n",
    "false_positive_rates = []\n",
    "holdout_prediction = []\n",
    "for thr in thresholds:\n",
    "    holdout_prediction = np.where(logit_predicted_probabilities_holdout < thr, 0, 1)\n",
    "    cm_thr = confusion_matrix(y_holdout, holdout_prediction, labels=[0,1])\n",
    "    cm[thr] = cm_thr\n",
    "    tn, fp, fn, tp = cm_thr.ravel()\n",
    "    true_positive_rates.append(tp/(tp+fn))\n",
    "    false_positive_rates.append(fp/(fp+tn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_thr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_thr.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr_fpr_for_thresholds = pd.DataFrame(\n",
    "    {'thresholds': thresholds,\n",
    "     'true_positive_rates': true_positive_rates,\n",
    "     'false_positive_rates': false_positive_rates})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr_fpr_for_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(tpr_fpr_for_thresholds, aes(x = 'false_positive_rates', y = 'true_positive_rates', color='thresholds'))\\\n",
    "        + labs(\n",
    "            x = \"False positive rate (1 - Specificity)\", \n",
    "            y = \"True positive rate (Sensitivity)\",\n",
    "            title = 'ROC curve for the best model (X4)'\n",
    "        )\\\n",
    "        + geom_point(size=2, alpha=0.8) + scale_color_continuous(trans = 'reverse')\\\n",
    "        + scale_x_continuous(limits=(0,1), breaks = seq(0, 1.01, by = 0.1))\\\n",
    "        + scale_y_continuous(limits=(0,1), breaks = seq(0, 1.01, by = 0.1))\\\n",
    "        + theme_bw()\\\n",
    "        +theme(legend_position=\"right\",\n",
    "            axis_text=element_text(size=10),\n",
    "            axis_title=element_text(size=10),\n",
    "            legend_text = element_text(size = 6),\n",
    "            legend_title = element_text(size = 6),\n",
    "            legend_key_size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(\n",
    "    data = tpr_fpr_for_thresholds,\n",
    "    x = 'false_positive_rates', \n",
    "    y = 'true_positive_rates', \n",
    "    marker = 'o')\n",
    "ax.set_title('ROC curve for the best model (X4)')\n",
    "ax.set_xlabel(\"False positive rate (1 - Specificity)\"), \n",
    "ax.set_ylabel(\"True positive rate (Sensitivity)\")\n",
    "ax.set_xticks([x/10 for x in range(0,10,1)])\n",
    "ax.set_yticks([x/10 for x in range(0,11,1)])\n",
    "ax.grid(True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**continuous ROC on holdout with best model (Logit 4)**\n",
    "\n",
    "*ggplot*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_roc_plot(y_holdout, logit_predicted_probabilities_holdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*seaborn*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_holdout, logit_predicted_probabilities_holdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = pd.DataFrame({\n",
    "    'fpr': fpr,\n",
    "    'tpr': tpr,\n",
    "    'thresholds': thresholds\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6,6))\n",
    "ax = sns.lineplot(\n",
    "    data = df_,\n",
    "    x = fpr, \n",
    "    y = tpr)\n",
    "ax.set_title('ROC curve for the best model (X4)')\n",
    "ax.set_xlabel(\"False positive rate (1 - Specificity)\"), \n",
    "ax.set_ylabel(\"True positive rate (Sensitivity)\")\n",
    "ax.set_xticks([x/10 for x in range(0,11,1)])\n",
    "ax.set_yticks([x/10 for x in range(0,11,1)])\n",
    "ax.plot(df_.fpr, df_.fpr, color = 'k')\n",
    "ax.grid(True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion table with different tresholds**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "default: the threshold 0.5 is used to convert probabilities to binary classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_class_prediction = best_model.predict(best_model_X_holdout)\n",
    "\n",
    "values, counts = np.unique(logit_class_prediction.tolist(), return_counts=True)\n",
    "print(values[0],' (no default): ',counts[0])\n",
    "print(values[1],' (default): ',counts[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "confusion matrix: summarize different type of errors and successfully predicted cases   \n",
    "positive = \"yes\": explicitly specify the positive case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_object1 = confusion_matrix(y_holdout, logit_class_prediction, labels=[0,1])\n",
    "cm1 = pd.DataFrame(cm_object1, \n",
    "    index=['Actual no default', 'Actual default'], \n",
    "    columns=['Predicted no default', 'Predicted default'])\n",
    "cm1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying different thresholds\n",
    "\n",
    "- 0.5 same as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_prediction = np.where(logit_predicted_probabilities_holdout < 0.5, 0, 1)\n",
    "cm_object1b = confusion_matrix(y_holdout, holdout_prediction, labels=[0,1])\n",
    "cm1b = pd.DataFrame(cm_object1b, \n",
    "    index=['Actual no default', 'Actual default'], \n",
    "    columns=['Predicted no default', 'Predicted default'])\n",
    "cm1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'FPR: {round(cm1b.iloc[0,1] / cm1b.iloc[0].sum(), 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- a sensible choice: mean of predicted probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_predicted_default_prob = np.mean(logit_predicted_probabilities_holdout)\n",
    "round(mean_predicted_default_prob, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_prediction = np.where(logit_predicted_probabilities_holdout < mean_predicted_default_prob, 0, 1)\n",
    "cm_object2 = confusion_matrix(y_holdout, holdout_prediction, labels=[0,1])\n",
    "cm2 = pd.DataFrame(cm_object2, \n",
    "    index=['Actul no defaul', 'Actual default'], \n",
    "    columns=['Predicted no default', 'Predicted default'])\n",
    "cm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'FPR: {round(cm2.iloc[0,1] / cm2.iloc[0].sum(), 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calibration curve\n",
    "\n",
    "How well do estimated vs actual event probabilities relate to each other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout = pd.concat([best_model_X_holdout, y_holdout], axis=1)\n",
    "holdout[\"best_logit_no_loss_pred\"] = logit_predicted_probabilities_holdout\n",
    "create_calibration_plot(\n",
    "    holdout, \n",
    "    # file_name = \"ch17-figure-1-logit-m4-calibration\",\n",
    "    prob_var='best_logit_no_loss_pred', \n",
    "    actual_var='default',\n",
    "    y_lab=\"Actual event probability\", \n",
    "    n_bins=10, \n",
    "    breaks=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC using a loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss function**: relative cost of of a false negative classification (as compared with a false positive classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FP = 1\n",
    "FN = 10\n",
    "cost = FN/FP\n",
    "\n",
    "# the prevalence, or the proportion of cases in the population (n.cases/(n.controls+n.cases))\n",
    "prevelance = y_train.sum()/len(y_train)\n",
    "prevelance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Draw ROC Curve and find optimal threshold with loss function**\n",
    "\n",
    "The optimal cut-off is the threshold that maximizes the distance to the identity (diagonal) line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_thresholds_cv = dict()\n",
    "expected_loss_cv = dict()\n",
    "fold5_threshold = dict()\n",
    "fold5_expected_loss = dict()\n",
    "fold5_all_coords = dict()\n",
    "\n",
    "for i, model_name in enumerate(logit_models):\n",
    "    best_thresholds = []\n",
    "    expected_loss =[]\n",
    "    if model_name != 'LASSO':\n",
    "        X = logit_model_vars[i]\n",
    "        c_index = 0\n",
    "    else:\n",
    "        X = normalized_logitvars\n",
    "        c_index = best_lambda_i\n",
    "    fold = 0\n",
    "    for train_index, test_index in k.split(X):\n",
    "        X_fold = X.iloc[test_index,:]\n",
    "        y_fold = y_train.iloc[test_index]\n",
    "        pred_fold = generate_fold_prediction(logit_models[model_name], X_fold, fold, c_index)\n",
    "        false_pos_rate, true_pos_rate, thresholds = roc_curve(y_fold, pred_fold)\n",
    "        optimal_threshold = sorted(list(zip(\n",
    "            np.abs(true_pos_rate + (1 - prevelance)/(cost * prevelance)*(1-false_pos_rate)),\\\n",
    "                                       thresholds)), key=lambda i: i[0], reverse=True)[0][1]\n",
    "        best_thresholds.append(optimal_threshold)\n",
    "        threshold_prediction = np.where(pred_fold < optimal_threshold, 0, 1)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_fold, threshold_prediction, labels=[0,1]).ravel()\n",
    "        curr_exp_loss = (fp*FP + fn*FN)/len(y_fold)\n",
    "        expected_loss.append(curr_exp_loss)\n",
    "        fold = fold+1\n",
    "\n",
    "    best_thresholds_cv[model_name] = np.mean(best_thresholds)\n",
    "    expected_loss_cv[model_name] = np.mean(expected_loss)\n",
    "\n",
    "    # for fold #5\n",
    "    fold5_threshold[model_name] = optimal_threshold\n",
    "    fold5_expected_loss[model_name] = curr_exp_loss\n",
    "\n",
    "    all_coords = pd.DataFrame({\n",
    "        'false_pos': false_pos_rate*sum(y_fold == 0),\n",
    "        'true_pos': true_pos_rate*sum(y_fold == 1),\n",
    "        'false_neg': sum(y_fold == 1) - true_pos_rate*sum(y_fold == 1),\n",
    "        'true_neg': sum(y_fold == 0) - false_pos_rate*sum(y_fold == 0),\n",
    "        'pos': sum(y_fold == 1),\n",
    "        'neg': sum(y_fold == 0),\n",
    "        'n': len(y_fold),\n",
    "        'thresholds': thresholds\n",
    "    })\n",
    "    \n",
    "    fold5_all_coords[model_name] = all_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold5_all_coords['X1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_summary2 = pd.DataFrame(best_thresholds_cv.items(),columns=['Model', 'Avg of optimal thresholds'])\n",
    "logit_summary2['Threshold for Fold5'] = fold5_threshold.values()\n",
    "logit_summary2['Avg expected loss'] = expected_loss_cv.values()\n",
    "logit_summary2['Expected loss for Fold5'] = fold5_expected_loss.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_summary2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss plot based on Fold5 in CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_plot = 'X1'\n",
    "create_loss_plot(\n",
    "    fold5_all_coords[model_to_plot], \n",
    "    fold5_threshold[model_to_plot], \n",
    "    fold5_expected_loss[model_to_plot])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC plot plot based on Fold5 in CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_plot = 'X1'\n",
    "create_roc_plot_with_optimal(fold5_all_coords[model_to_plot], fold5_threshold[model_to_plot])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pick best model based on average expected loss\n",
    "\n",
    "### X4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_logit_optimal_treshold = best_thresholds_cv[\"X4\"]\n",
    "best_logit_optimal_treshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get expected loss on holdout**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_treshold = np.where(logit_predicted_probabilities_holdout < best_logit_optimal_treshold, 0, 1)\n",
    "tn, fp, fn, tp = confusion_matrix(y_holdout, holdout_treshold, labels=[0,1]).ravel()\n",
    "expected_loss_holdout = (fp*FP + fn*FN)/len(y_holdout)\n",
    "round(expected_loss_holdout, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_object3 = confusion_matrix(y_holdout, holdout_treshold, labels=[0,1])\n",
    "cm3 = pd.DataFrame(cm_object3, \n",
    "    index=['Actul no defaul', 'Actual default'], \n",
    "    columns=['Predicted no default', 'Predicted default'])\n",
    "cm3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'FPR: {round(cm3.iloc[0,1] / cm3.iloc[0].sum(), 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction with random forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfvars_train = rfvars.iloc[index_train]\n",
    "rfvars_holdout = rfvars.iloc[index_holdout]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfvars_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "data_for_graph = rfvars_train[['sales_mil','profit_loss_year','foreign_management']]\n",
    "rf_for_graph = DecisionTreeClassifier(\n",
    "    ccp_alpha=0.0028, \n",
    "    min_samples_leaf=100, \n",
    "    max_depth=3, \n",
    "    random_state=41).fit(\n",
    "    data_for_graph, \n",
    "    y_train)\n",
    "\n",
    "plt.figure()\n",
    "plt.figure(figsize=(11,11))\n",
    "plot_tree(rf_for_graph, \n",
    "          feature_names = data_for_graph.columns, \n",
    "          filled=True, \n",
    "          rounded=True, \n",
    "          proportion=True, \n",
    "          fontsize = 10)\n",
    "plt.title(\"Decision trees\")\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability forest\n",
    "\n",
    "Split by gini, ratio of 1's in each tree, average over trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {'max_features': [5, 6, 7],\n",
    "        'criterion':['gini'],\n",
    "        'min_samples_split': [11, 16]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_forest = RandomForestClassifier(\n",
    "    random_state=20240205, \n",
    "    n_estimators=500, \n",
    "    oob_score=True)\n",
    "\n",
    "prob_forest_grid = GridSearchCV(\n",
    "    prob_forest, \n",
    "    grid, \n",
    "    cv=k, \n",
    "    refit='accuracy',\n",
    "    scoring = ['accuracy', 'roc_auc', 'neg_brier_score'], \n",
    "    n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "prob_forest_fit = prob_forest_grid.fit(rfvars_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CV summary table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_forest_fit.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our grid has 3x2=6 elements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_accuracy = np.zeros([6])\n",
    "for i in range(5):\n",
    "    cv_accuracy = cv_accuracy + prob_forest_fit.cv_results_['split' + str(i) + '_test_accuracy']\n",
    "cv_accuracy = cv_accuracy/5\n",
    "cv_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_forest_fit.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_auc = np.zeros([6])\n",
    "for i in range(5):\n",
    "    cv_auc = cv_auc + prob_forest_fit.cv_results_['split' + str(i) + '_test_roc_auc']\n",
    "cv_auc = cv_auc/5\n",
    "cv_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_rmse = np.zeros([6])\n",
    "for i in range(5):\n",
    "    cv_rmse = cv_rmse +np.sqrt(-1*(prob_forest_fit.cv_results_['split' + str(i) + '_test_neg_brier_score'])).tolist()\n",
    "cv_rmse = cv_rmse/5\n",
    "cv_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_forest_cv_results = pd.DataFrame({\n",
    "    'max_features': prob_forest_fit.cv_results_['param_max_features'],\n",
    "    'min_samples_split': prob_forest_fit.cv_results_['param_min_samples_split'],\n",
    "    'cv_accuracy': cv_accuracy,\n",
    "    'cv_auc': cv_auc,\n",
    "    'cv_rmse': cv_rmse\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_forest_cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimal parameter values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_max_features = prob_forest_fit.best_params_['max_features']\n",
    "best_min_sample_split = prob_forest_fit.best_params_['min_samples_split']\n",
    "prob_forest_fit.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Average RMSE and AUC over folds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_forest_best_results = prob_forest_cv_results[\n",
    "    (prob_forest_cv_results.max_features == best_max_features) & \n",
    "    (prob_forest_cv_results.min_samples_split == best_min_sample_split)]\n",
    "prob_forest_best_results_index = prob_forest_best_results.index.values[0]\n",
    "\n",
    "CV_RMSE['rf_p'] = prob_forest_best_results.cv_rmse.values[0]\n",
    "CV_AUC['rf_p'] = prob_forest_best_results.cv_auc.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get fold level RMSE and AUC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_RMSE_folds_rf_p = list()\n",
    "\n",
    "for i in range(5):\n",
    "    rmse = np.sqrt(-1*(prob_forest_fit.cv_results_['split' + str(i) + '_test_neg_brier_score'])).tolist()[prob_forest_best_results_index]\n",
    "    CV_RMSE_folds_rf_p.append(rmse)\n",
    "\n",
    "CV_RMSE_folds['rf_p'] = CV_RMSE_folds_rf_p\n",
    "\n",
    "CV_AUC_folds_rf_p = list()\n",
    "\n",
    "for i in range(5):\n",
    "    rmse = prob_forest_fit.cv_results_['split' + str(i) + '_test_roc_auc'][prob_forest_best_results_index]\n",
    "    CV_AUC_folds_rf_p.append(rmse)\n",
    "\n",
    "CV_AUC_folds['rf_p'] = CV_AUC_folds_rf_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(CV_AUC_folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now use loss function and search for best thresholds and expected loss over folds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_thresholds = list()\n",
    "expected_loss = list()\n",
    "\n",
    "fold = 0\n",
    "for train_index, test_index in k.split(rfvars_train):\n",
    "    X_fold = rfvars_train.iloc[test_index,:]\n",
    "    y_fold = y_train.iloc[test_index]\n",
    "    \n",
    "    X_fold_train = rfvars_train.iloc[train_index,:]\n",
    "    y_fold_train = y_train.iloc[train_index]\n",
    "    \n",
    "    prob_forest_best = RandomForestClassifier(\n",
    "        random_state=20240205, \n",
    "        n_estimators=500, \n",
    "        oob_score=True,\n",
    "        criterion = 'gini', \n",
    "        max_features = best_max_features, min_samples_split = best_min_sample_split)\n",
    "    \n",
    "    prob_forest_best_fold = prob_forest_best.fit(X_fold_train, y_fold_train)\n",
    "    pred_fold = prob_forest_best_fold.predict_proba(X_fold)[:,1]\n",
    "\n",
    "    false_pos_rate, true_pos_rate, threshold = roc_curve(y_fold, pred_fold)\n",
    "    \n",
    "    best_threshold = sorted(\n",
    "        list(\n",
    "            zip(\n",
    "                np.abs(\n",
    "                    true_pos_rate + (1 - prevelance)/(cost * prevelance)*(1-false_pos_rate)\n",
    "                ),\n",
    "                threshold\n",
    "            )\n",
    "        ), \n",
    "        key=lambda x: x[0], reverse=True)[0][1]\n",
    "    \n",
    "    best_thresholds.append(best_threshold)\n",
    "    \n",
    "    threshold_prediction = np.where(pred_fold < best_threshold, 0, 1)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_fold, threshold_prediction, labels=[0,1]).ravel()\n",
    "    curr_exp_loss = (fp*FP + fn*FN)/len(y_fold)\n",
    "    expected_loss.append(curr_exp_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold5_threshold_rf = best_threshold\n",
    "fold5_expected_loss_rf = curr_exp_loss\n",
    "\n",
    "all_coords_rf = pd.DataFrame({\n",
    "    'false_pos': false_pos_rate*sum(y_fold == 0),\n",
    "    'true_pos': true_pos_rate*sum(y_fold == 1),\n",
    "    'false_neg': sum(y_fold == 1) - true_pos_rate*sum(y_fold == 1),\n",
    "    'true_neg': sum(y_fold == 0) - false_pos_rate*sum(y_fold == 0),\n",
    "    'pos': sum(y_fold == 1),\n",
    "    'neg': sum(y_fold == 0),\n",
    "    'n': len(y_fold),\n",
    "    'thresholds': threshold\n",
    "})\n",
    "\n",
    "all_coords_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold5_threshold_rf = best_threshold\n",
    "fold5_expected_loss_rf = curr_exp_loss\n",
    "\n",
    "all_coords_rf = pd.DataFrame({\n",
    "    'false_pos': false_pos_rate*sum(y_fold == 0),\n",
    "    'true_pos': true_pos_rate*sum(y_fold == 1),\n",
    "    'false_neg': sum(y_fold == 1) - true_pos_rate*sum(y_fold == 1),\n",
    "    'true_neg': sum(y_fold == 0) - false_pos_rate*sum(y_fold == 0),\n",
    "    'pos': sum(y_fold == 1),\n",
    "    'neg': sum(y_fold == 0),\n",
    "    'n': len(y_fold),\n",
    "    'thresholds': threshold\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_loss_cv['rf_p'] = np.mean(expected_loss)\n",
    "best_thresholds_cv['rf_p'] = np.mean(best_thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_summary = pd.DataFrame(\n",
    "    {'CV RMSE': [round(CV_RMSE['rf_p'], 3)],\n",
    "     'CV AUC': [round(CV_AUC['rf_p'], 3)],\n",
    "     'Avg of optimal thresholds': [round(best_thresholds_cv['rf_p'], 3)],\n",
    "     'Threshold for Fold5': [round(best_threshold, 3)],\n",
    "     'Avg expected loss': [round(expected_loss_cv['rf_p'], 3)],\n",
    "     'Expected loss for Fold5': [round(curr_exp_loss, 3)]})\n",
    "\n",
    "rf_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plots based on Fold5 in CV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_loss_plot(all_coords_rf, fold5_threshold_rf, fold5_expected_loss_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_roc_plot_with_optimal(all_coords_rf, fold5_threshold_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Take model to holdout and estimate RMSE, AUC and expected loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_forest_fit_best = prob_forest_fit.best_estimator_\n",
    "rf_predicted_probabilities_holdout = prob_forest_fit_best.predict_proba(rfvars_holdout)[:,1]\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_holdout, rf_predicted_probabilities_holdout))\n",
    "round(rmse_rf, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ROC AUC  on holdout**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_rf = roc_auc_score(y_holdout, rf_predicted_probabilities_holdout)\n",
    "round(auc_rf, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected loss on holdout**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_treshold = np.where(rf_predicted_probabilities_holdout < best_thresholds_cv['rf_p'], 0, 1)\n",
    "tn, fp, fn, tp = confusion_matrix(y_holdout, holdout_treshold, labels=[0,1]).ravel()\n",
    "expected_loss_holdout = (fp*FP + fn*FN)/len(y_holdout)\n",
    "round(expected_loss_holdout, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification forest\n",
    "\n",
    "Split by Gini, majority vote in each tree, majority vote over trees\n",
    "\n",
    "Show expected loss with classification RF and default majority voting to compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvars['rf_p'] = len(rfvars.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_results = pd.DataFrame({\"Model\": list(nvars.keys()),\n",
    "                              \"Number of predictors\": list(nvars.values()),\n",
    "                              \"CV RMSE\": list(CV_RMSE.values()),\n",
    "                              \"CV AUC\": list(CV_AUC.values()),\n",
    "                              \"CV threshold\": list(best_thresholds_cv.values()),\n",
    "                              \"CV expected Loss\": list(expected_loss_cv.values())\n",
    "                               })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
